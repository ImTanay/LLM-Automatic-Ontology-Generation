{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs for producing taxonomies of research topics\n",
    "\n",
    "## Classifying relationships with Chain of Thought\n",
    "This code performs requests to Amazon Bedrock for building taxonomies of research topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import requests, json, os, io, re, base64, random, time, csv, datetime\n",
    "from IPython.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from bedrock import BedrockWrapper\n",
    "from gpt import GPTWrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising Amazon Bedrock Wrapper & more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use = \"\" #bedrock or gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use == \"bedrock\":\n",
    "    wrapper = BedrockWrapper(model=\"YOUR_MODEL\")\n",
    "elif use == \"gpt\":\n",
    "    wrapper = GPTWrapper(api_key=\"YOUR_API_KEY\", model=\"YOUR_MODEL\")\n",
    "\n",
    "global conversation_history\n",
    "conversation_history = \"\"\n",
    "botname = \"assistant\"\n",
    "username = \"user\"\n",
    "\n",
    "results=[]\n",
    "results_history=[]\n",
    "\n",
    "GOLD_STANDARD_FILE = \"../dataset/IEEE-Rel-1K.csv\"#\"TOY-40\"#\"GS_2650\"\n",
    "RESULTS_FOLDER     = \"../results\"\n",
    "DYNAMIC = True # it changes the file where to save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPceVlmzd--O",
    "outputId": "b308e5c0-a664-447c-d4e5-50cb5864a1d5"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(topic1, topic2): \n",
    "    \n",
    "    prompt1_template = \"\"\"\n",
    "    \n",
    "Classify the relationship between '[TOPIC-A]' and '[TOPIC-B]' by applying the following relationship definitions:\n",
    "1. '[TOPIC-A]' is-broader-than '[TOPIC-B]' if '[TOPIC-A]' is a super-category of '[TOPIC-B]', that is '[TOPIC-B]' is a type, a branch, or a specialised aspect of '[TOPIC-A]' or that '[TOPIC-B]' is a tool or a methodology mostly used in the context of '[TOPIC-A]' (e.g., car is-broader-than wheel).\n",
    "2. '[TOPIC-A]' is-narrower-than '[TOPIC-B]' if '[TOPIC-A]' is a sub-category of '[TOPIC-B]', that is '[TOPIC-A]' is a type, a branch, or a specialised aspect of '[TOPIC-B]' or that '[TOPIC-A]' is a tool or a methodology mostly used in the context of '[TOPIC-B]' (e.g., wheel is-narrower-than car).\n",
    "3. '[TOPIC-A]' is-same-as-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' are synonymous terms denoting a very similar concept (e.g., 'beautiful' is-same-as-than 'attractive'), including when one is the plural form of the other (e.g., cat is-same-as-than cats).\n",
    "4. '[TOPIC-A]' is-other-than '[TOPIC-B]' if '[TOPIC-A]' and '[TOPIC-B]' either have no direct relationship or share a different kind of relationship that does not fit into the other defined relationships.\n",
    "\n",
    "Think step by step by following these sequential instructions:\n",
    "1) Provide a precise definition for '[TOPIC-A]'.\n",
    "2) Provide a precise definition for '[TOPIC-B]'.\n",
    "3) Formulate a sentence that includes both '[TOPIC-A]' and '[TOPIC-B]'.\n",
    "4) Discuss '[TOPIC-A]' and '[TOPIC-B]' usage and relationship (is-narrower-than, is-broader-than, is-same-as-than, or is-other-than).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prompt2_template = \"\"\"\n",
    "    \n",
    "Given the previous discussion, determine which one of the following statements is correct:\n",
    "1. '[TOPIC-A]' is-broader-than '[TOPIC-B]'\n",
    "2. '[TOPIC-B]' is-narrower-than '[TOPIC-A]'\n",
    "3. '[TOPIC-A]' is-narrower-than '[TOPIC-B]'\n",
    "4. '[TOPIC-B]' is-broader-than '[TOPIC-A]'\n",
    "5. '[TOPIC-A]' is-same-as-than '[TOPIC-B]'\n",
    "6. '[TOPIC-A]' is-other-than '[TOPIC-B]'\n",
    "\n",
    "Answer by only stating the number of the correct statement.\n",
    "\n",
    "    \"\"\"\n",
    "   \n",
    "    prompt1 = prompt1_template.replace(\"[TOPIC-A]\",topic1).replace(\"[TOPIC-B]\",topic2)\n",
    "    prompt2 = prompt2_template.replace(\"[TOPIC-A]\",topic1).replace(\"[TOPIC-B]\",topic2)\n",
    "    \n",
    "    return prompt1, prompt2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Routines for classification and parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_for_mistral_amazon_bedrock(text:str, verbose:bool=False)->str:\n",
    "    last = \"6\"\n",
    "    \n",
    "    text = text.strip()\n",
    "    text = text[:text.rfind(\"Explanation:\")] # remove the bit with explanation\n",
    "    if text != None and text != '':\n",
    "        splitted = text.splitlines()\n",
    "        if verbose: print(splitted)\n",
    "        for line in splitted:\n",
    "            if len(line) > 0 and bool(re.search(r'\\d', line)): # the last branch checks if it contains numbers\n",
    "                last = line\n",
    "                break            \n",
    "    return last\n",
    "\n",
    "\n",
    "def parser_for_cohere_amazon_bedrock(text:str, verbose:bool=False)->str:\n",
    "    last = \"6\"\n",
    "    \n",
    "    text = text.strip()\n",
    "    if text != None and text != '':\n",
    "        splitted = text.splitlines()\n",
    "        if verbose: print(splitted)\n",
    "        for line in splitted:\n",
    "            if len(line) > 0 and bool(re.search(r'\\d', line)): # the last branch checks if it contains numbers\n",
    "                try:\n",
    "                    last = re.search('\\d+.', line).group()  \n",
    "                except:\n",
    "                    last = line\n",
    "                    print(line)\n",
    "                break\n",
    "    return last\n",
    "\n",
    "\n",
    "def gpt(text:str, verbose:bool=False)->str:\n",
    "    text = text.strip()\n",
    "    if text == None or text == '':\n",
    "        return \"6\"\n",
    "    splitted = text.splitlines()\n",
    "    if verbose: print(splitted)\n",
    "    last = splitted[len(splitted)-1]\n",
    "    return last\n",
    "    \n",
    "\n",
    "\n",
    "## Does the conversion of numbers in the actual relationship\n",
    "def simple_parser(text:str)->str:\n",
    "    if use == \"bedrock\":\n",
    "        last = parser_for_mistral_amazon_bedrock(text)\n",
    "    elif use == \"gpt\":\n",
    "        last = gpt(text)\n",
    "\n",
    "\n",
    "    numbers = re.findall(r'\\d', last)\n",
    "    last_number = numbers[-1] # I get the last number, should there be more than one!\n",
    "    \n",
    "    if \"1\" in last_number or  \"2\" in last_number : return \"broader\"\n",
    "    if \"3\" in last_number or  \"4\" in last_number: return \"narrower\"\n",
    "    if \"5\" in last_number or  \"synonymous\" in last_number: return \"same-as\"\n",
    "    if \"6\" in last_number or  \"different\" in last_number : return \"other\"\n",
    "    else : return \"other\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_message(user_message, wrapper, verbose = False):\n",
    "    global conversation_history\n",
    "    if conversation_history == \"\":\n",
    "        new_user_message = f\"{username}: {user_message}\\n\\n{botname}:\"\n",
    "    else: \n",
    "        new_user_message = f\"{conversation_history}\\n\\n{username}: {user_message}\\n\\n{botname}:\"\n",
    "\n",
    "\n",
    "    response_text = wrapper.invoke_model(new_user_message, verbose = verbose, test = False)\n",
    "    \n",
    "    conversation_history = f\"{conversation_history}\\n\\n{username}: {user_message}\\n\\n{botname}: {response_text}\\n\" # Update the conversation history with the user message and bot response\n",
    "\n",
    "    with open(f'conv_history_{botname}_terminal.txt', \"a\") as f:\n",
    "        f.write(f\"{username}: {user_message}\\n\\n{botname}: {response_text}\\n\") # Append conversation to text file\n",
    "\n",
    "    return response_text\n",
    "        \n",
    "        \n",
    "    \n",
    "# this is the core function\n",
    "def classify(topic1, topic2, wrapper, max_num_continue=3, verbose=True, answer_min_size=200)  :\n",
    "    prompt1, prompt2 = generate_prompt(topic1,topic2) # we create both prompts\n",
    "    \n",
    "    words_p1 = len(prompt1.split())\n",
    "    r = handle_message(prompt1, wrapper, verbose = False) # submit prompt 1\n",
    "   \n",
    "    \n",
    "    if verbose:\n",
    "        print(\"response:\" + r)\n",
    "        print(\"response len:\", len(r) )\n",
    "         \n",
    "            \n",
    "    result = handle_message(prompt2, wrapper, verbose = False) # submit prompt 1\n",
    "    if verbose: print(result)\n",
    "    return simple_parser(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(f'{GOLD_STANDARD_FILE}.csv', encoding = \"UTF-8\", keep_default_na=False)\n",
    "dataset = dataset[[\"subject\", \"object\",\"original_label\"]]\n",
    "mapping = {\"supertopic\":\"broader\", \"subtopic\":\"narrower\", \"same_as\":\"same-as\", \"not_related\":\"other\"}\n",
    "dataset[\"original_label\"] = dataset[\"original_label\"].apply(lambda x: mapping[x] if x in mapping else x)\n",
    "print(f\"Total number of rows: {len(dataset)}\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating CSV \n",
    "For hosting our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "# create file name\n",
    "current_time = datetime.datetime.now()\n",
    "ttime = str(current_time).split(\".\")[0].replace(\" \",\"_\").replace(\":\",\"-\") if DYNAMIC else \"final\"\n",
    "sides = \"DOUBLE-SIDED\" if BOTH_ORDER else \"SINGLE-SIDED\"\n",
    "num_prompts = \"DOUBLE-PROMPT\"\n",
    "results_file_name= f'{RESULTS_FOLDER}/GPT4/{GOLD_STANDARD_FILE}_{ttime}.csv'\n",
    "print(f\"Results will be saved in {results_file_name}\")\n",
    "\n",
    "###########################################\n",
    "# initialize csv\n",
    "\n",
    "file=open(results_file_name, 'w', newline='')\n",
    "writer = csv.writer(file)\n",
    "writer.writerow(['subject', 'object', 'original_label', 'predicted_label', 'Predicted1', 'Predicted2', 'log', 'tokens_used'])\n",
    "file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over the relationships of the Gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_processed = 0\n",
    "VERBOSE = False\n",
    "START = 0\n",
    "END = len(dataset)\n",
    "\n",
    "with tqdm(total=END-START) as pbar:\n",
    "    for idx, row in dataset.iterrows():\n",
    "        if idx >= START and idx < END:\n",
    "            topic1 = row[\"subject\"]\n",
    "            topic2 = row[\"object\"]\n",
    "            target = row[\"original_label\"]\n",
    "        \n",
    "            result1 = classify(topic1, topic2, wrapper, verbose=False, max_num_continue=0)\n",
    "            conversation_history1 = conversation_history\n",
    "            conversation_history = ''\n",
    "             \n",
    "            result2 = classify(topic2, topic1, wrapper, verbose=False, max_num_continue=0)\n",
    "            conversation_history2 = conversation_history\n",
    "            conversation_history = conversation_history1 + conversation_history2\n",
    "\n",
    "            if result1 == 'broader' and result2 == 'narrower':\n",
    "                predicted = 'broader'\n",
    "            elif result1 == 'narrower' and result2 == 'broader':\n",
    "                predicted = 'narrower'\n",
    "            elif (result1 == 'narrower' and result2 == 'narrower') or (result1 == 'broader' and result2 == 'broader'):\n",
    "                if len(topic1) <= len(topic2):\n",
    "                    predicted = 'broader'  # if contradiction broader is the shorter\n",
    "                else:\n",
    "                    predicted = 'narrower'\n",
    "            else:  # different\n",
    "                if result1 == 'same-as' or result2 == 'same-as':\n",
    "                    predicted = 'same-as'  # if one of them is same-as, keep it\n",
    "                elif result1 == 'broader' and result2 == 'other' or result1 == 'other' and result2 == 'narrower':\n",
    "                    predicted = 'broader'  # direction wins over other\n",
    "                elif result1 == 'narrower' and result2 == 'other' or result1 == 'other' and result2 == 'broader':\n",
    "                    predicted = 'narrower'  # direction wins over other\n",
    "                else:\n",
    "                    predicted = result1\n",
    "\n",
    "            results.append(predicted)\n",
    "            results_history.append(conversation_history)\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(f\"Computed {idx} iterations\")\n",
    "\n",
    "            if VERBOSE:\n",
    "                if predicted == target:\n",
    "                    print(\"Matched\", end=\" \")\n",
    "\n",
    "            writer.writerow([topic1, topic2, target, predicted, result1, result2, conversation_history])\n",
    "            file.flush()\n",
    "            relationship_processed += 1\n",
    "\n",
    "            # resetting conversation\n",
    "            conversation_history = \"\"\n",
    "            pbar.update(1)\n",
    "\n",
    "display(HTML(f\"\"\"<a href=\"{results_file_name}\">To see these results click here.</a>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
